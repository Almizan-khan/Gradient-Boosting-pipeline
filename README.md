# Gradient-Boosting-pipeline


Finally completed my full Gradient Boosting training end to end on a live Kaggle Playground competition

Worked on Predicting Student Test Scores using tabular data and focused completely on doing things the right way not shortcuts

Used CatBoost regression with proper k fold cross validation early stopping and RMSE based evaluation

Handled categorical features correctly avoided data leakage and tracked validation performance fold by fold instead of trusting a single split

Final cross validation RMSE came out to be 8 75719 which is very close to leaderboard performance

Feature importance analysis showed that study hours class attendance sleep quality and study method drive most of the prediction while demographic features had minimal impact which also makes real world sense

This project helped me deeply understand how gradient boosting actually learns how residuals are corrected over iterations and why cross validation matters more than leaderboard chasing

Still learning still improving but this one gave me real confidence in my fundamentals of machine learning and gradient boosting
